{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IDRR_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'/data/whsun/idrr/data/raw/pdtb2.p1.csv'\n",
    "df = IDRRDataFrames(\n",
    "    data_name='pdtb2',\n",
    "    data_level='top',\n",
    "    data_relation='Implicit',\n",
    "    data_path=data_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['arg1', 'arg2', 'conn1', 'conn2', 'conn1sense1', 'conn1sense2',\n",
       "       'conn2sense1', 'conn2sense2', 'relation', 'split', 'Section',\n",
       "       'FileNumber', 'label11', 'label11id', 'label12', 'label12id', 'label21',\n",
       "       'label21id', 'label22', 'label22id', 'ans_word1', 'ans_word1id',\n",
       "       'ans_word2', 'ans_word2id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arg1           In an Oct. 19 review of \"The Misanthrope\" at C...\n",
       "arg2                                      Ms. Haag plays Elianti\n",
       "conn1                                                    however\n",
       "conn2                                                        NaN\n",
       "conn1sense1                    Comparison.Contrast.Juxtaposition\n",
       "conn1sense2                                                  NaN\n",
       "conn2sense1                                                  NaN\n",
       "conn2sense2                                                  NaN\n",
       "relation                                                Implicit\n",
       "split                                                      train\n",
       "Section                                                        2\n",
       "FileNumber                                                     0\n",
       "label11                                               Comparison\n",
       "label11id                                                      0\n",
       "label12                                                     <NA>\n",
       "label12id                                                   <NA>\n",
       "label21                                                     <NA>\n",
       "label21id                                                   <NA>\n",
       "label22                                                     <NA>\n",
       "label22id                                                   <NA>\n",
       "ans_word1                                                however\n",
       "ans_word1id                                                    3\n",
       "ans_word2                                                   <NA>\n",
       "ans_word2id                                                 <NA>\n",
       "Name: 4976, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.train_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parquet格式化数据 (以qwen3为例)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Task\\nYou are an expert in the field of implicit discourse relations. Your task is to determine the semantic-logical relationship between two given text segments and select the most appropriate relation label. Output only one of A, B, C, or D, and enclose it in \\\\boxed{{}}.\\n\\n### Relations\\n{relation_terms}\\n\\n### Segments\\nText segment 1: {arg1}\\nText segment 2: {arg2}\\n\\nYour answer:\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "prompt_tmeplate = read_txt(\"/data/whsun/idrr/prompts/rl_base.txt\")\n",
    "prompt_tmeplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_source': 'pdtb',\n",
       " 'prompt': [{'content': '<|im_start|>user\\n### Task\\nYou are an expert in the field of implicit discourse relations. Your task is to determine the semantic-logical relationship between two given text segments and select the most appropriate relation label. Output only one of A, B, C, or D, and enclose it in \\\\boxed{}.\\n\\n### Relations\\nA. Comparison\\nB. Contingency\\nC. Expansion\\nD. Temporal\\n\\n### Segments\\nText segment 1: In an Oct. 19 review of \"The Misanthrope\" at Chicago\\'s Goodman Theatre (\"Revitalized Classics Take the Stage in Windy City,\" Leisure & Arts), the role of Celimene, played by Kim Cattrall, was mistakenly attributed to Christina Haag\\nText segment 2: Ms. Haag plays Elianti\\n\\nYour answer:\\n<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "   'role': 'user'}],\n",
       " 'reward_model': {'ground_truth': 'A'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/whsun/pretrained_models/Qwen/Qwen3-0.6B\")\n",
    "\n",
    "def get_rl_data(data_source: str, df, label_list):\n",
    "    rl_data = []\n",
    "    relation_terms = '\\n'.join([f\"{chr(65 + i)}. {label}\" for i, label in enumerate(label_list)])\n",
    "    label2alpha = {label: chr(65 + i) for i, label in enumerate(label_list)}\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = prompt_tmeplate.format(\n",
    "            relation_terms=relation_terms,\n",
    "            arg1=row['arg1'],\n",
    "            arg2=row['arg2'],\n",
    "        )\n",
    "        grounth_truth_alpha = label2alpha[row[\"label11\"]]\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        rl_data.append(\n",
    "            {\n",
    "                \"data_source\": data_source,\n",
    "                \"prompt\": [{\"content\": prompt_text, \"role\": \"user\"}],\n",
    "                \"reward_model\": {\"ground_truth\": grounth_truth_alpha},\n",
    "            }\n",
    "        )\n",
    "    return Dataset.from_list(rl_data, split=\"train\")\n",
    "\n",
    "train_rl_dataset = get_rl_data(\"pdtb\", df.train_df, df.label_list)\n",
    "train_rl_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['data_source', 'prompt', 'reward_model'],\n",
       "     num_rows: 1183\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['data_source', 'prompt', 'reward_model'],\n",
       "     num_rows: 1046\n",
       " }))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_rl_dataset = get_rl_data(\"pdtb\", df.dev_df, df.label_list)\n",
    "test_rl_dataset = get_rl_data(\"pdtb\", df.test_df, df.label_list)\n",
    "dev_rl_dataset, test_rl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:00<00:00, 80.08ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 496.96ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 543.34ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "712503"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rl_dataset.to_parquet(\"/data/whsun/idrr/data/rl/verl/pdtb2/top/qwen3_train.parquet\")\n",
    "dev_rl_dataset.to_parquet(\"/data/whsun/idrr/data/rl/verl/pdtb2/top/qwen3_dev.parquet\")\n",
    "test_rl_dataset.to_parquet(\"/data/whsun/idrr/data/rl/verl/pdtb2/top/qwen3_test.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
