{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IDRR_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'/data/whsun/idrr/data/raw/pdtb2.p1.csv'\n",
    "df = IDRRDataFrames(\n",
    "    data_name='pdtb2',\n",
    "    data_level='second',\n",
    "    data_relation='Implicit',\n",
    "    data_path=data_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['arg1', 'arg2', 'conn1', 'conn2', 'conn1sense1', 'conn1sense2',\n",
       "       'conn2sense1', 'conn2sense2', 'relation', 'split', 'Section',\n",
       "       'FileNumber', 'label11', 'label11id', 'label12', 'label12id', 'label21',\n",
       "       'label21id', 'label22', 'label22id', 'ans_word1', 'ans_word1id',\n",
       "       'ans_word2', 'ans_word2id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Contingency.Cause': 3227,\n",
       "         'Expansion.Conjunction': 2805,\n",
       "         'Expansion.Restatement': 2376,\n",
       "         'Comparison.Contrast': 1566,\n",
       "         'Expansion.Instantiation': 1061,\n",
       "         'Temporal.Asynchronous': 517,\n",
       "         'Expansion.List': 330,\n",
       "         'Comparison.Concession': 180,\n",
       "         'Temporal.Synchrony': 147,\n",
       "         'Expansion.Alternative': 146,\n",
       "         'Contingency.Pragmatic cause': 51})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(df.train_df['label11'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parquet格式化数据 (以qwen3为例)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Task\\nYou are an expert in the field of implicit discourse relations. Your task is to determine the semantic-logical relationship between two given text segments and select the most appropriate relation label. Output only one of A, B, C, or D, and enclose it in \\\\boxed{{}}.\\n\\n### Relations\\n{relation_terms}\\n\\n### Segments\\nText segment 1: {arg1}\\nText segment 2: {arg2}\\n\\nYour answer:\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "prompt_tmeplate = read_txt(\"/data/whsun/idrr/prompts/rl_base.txt\")\n",
    "prompt_tmeplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_source': 'pdtb',\n",
       " 'prompt': [{'content': '<|im_start|>user\\n### Task\\nYou are an expert in the field of implicit discourse relations. Your task is to determine the semantic-logical relationship between two given text segments and select the most appropriate relation label. Output only one of A, B, C, or D, and enclose it in \\\\boxed{}.\\n\\n### Relations\\nA. Comparison\\nB. Contingency\\nC. Expansion\\nD. Temporal\\n\\n### Segments\\nText segment 1: In an Oct. 19 review of \"The Misanthrope\" at Chicago\\'s Goodman Theatre (\"Revitalized Classics Take the Stage in Windy City,\" Leisure & Arts), the role of Celimene, played by Kim Cattrall, was mistakenly attributed to Christina Haag\\nText segment 2: Ms. Haag plays Elianti\\n\\nYour answer:\\n<|im_end|>\\n<|im_start|>assistant\\n',\n",
       "   'role': 'user'}],\n",
       " 'reward_model': {'ground_truth': 'A'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/whsun/pretrained_models/Qwen/Qwen3-0.6B\")\n",
    "\n",
    "def get_rl_data(data_source: str, df, label_list):\n",
    "    rl_data = []\n",
    "    relation_terms = '\\n'.join([f\"{chr(65 + i)}. {label}\" for i, label in enumerate(label_list)])\n",
    "    label2alpha = {label: chr(65 + i) for i, label in enumerate(label_list)}\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = prompt_tmeplate.format(\n",
    "            relation_terms=relation_terms,\n",
    "            arg1=row['arg1'],\n",
    "            arg2=row['arg2'],\n",
    "        )\n",
    "        grounth_truth_alpha = label2alpha[row[\"label11\"]]\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        rl_data.append(\n",
    "            {\n",
    "                \"data_source\": data_source,\n",
    "                \"prompt\": [{\"content\": prompt_text, \"role\": \"user\"}],\n",
    "                \"reward_model\": {\"ground_truth\": grounth_truth_alpha},\n",
    "            }\n",
    "        )\n",
    "    return Dataset.from_list(rl_data, split=\"train\")\n",
    "\n",
    "train_rl_dataset = get_rl_data(\"pdtb\", df.train_df, df.label_list)\n",
    "train_rl_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['data_source', 'prompt', 'reward_model'],\n",
       "     num_rows: 1183\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['data_source', 'prompt', 'reward_model'],\n",
       "     num_rows: 1046\n",
       " }))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_rl_dataset = get_rl_data(\"pdtb\", df.dev_df, df.label_list)\n",
    "test_rl_dataset = get_rl_data(\"pdtb\", df.test_df, df.label_list)\n",
    "dev_rl_dataset, test_rl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:00<00:00, 80.08ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 496.96ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 543.34ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "712503"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rl_dataset.to_parquet(\"/data/whsun/idrr/data/rl/verl/pdtb2/top/qwen3_train.parquet\")\n",
    "dev_rl_dataset.to_parquet(\"/data/whsun/idrr/data/rl/verl/pdtb2/top/qwen3_dev.parquet\")\n",
    "test_rl_dataset.to_parquet(\"/data/whsun/idrr/data/rl/verl/pdtb2/top/qwen3_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将label转化为关系定义进行sft\n",
    "1. 将二级label转为关系定义\n",
    "2. 模型1：论元对 -> 关系定义\n",
    "3. 模型2：模型1给出文本 -> label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Briefly describe the relationship between two arguments.\\nArg1: {arg1}\\nArg2: {arg2}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "prompt_tmeplate = read_txt(\"/data/whsun/idrr/prompts/arg2def.txt\")\n",
    "prompt_tmeplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /data/whsun/idrr/data/arg2def/pdtb2/aplaca/train.json\n",
      "Data saved to /data/whsun/idrr/data/arg2def/pdtb2/aplaca/test.json\n"
     ]
    }
   ],
   "source": [
    "sense2df = {\n",
    "    \"Temporal\": \"The situations described in the arguments are related temporally.\",\n",
    "    \"Temporal.Asynchronous\": \"One envent is described as preceding the other.\",\n",
    "    \"Temporal.Synchrony\": \"There is some degree of temporal overlap between the events described by the arguments.\",\n",
    "    \"Contingency\": \"One of the situations described in Arg1 and Arg2 causally influences the other.\",\n",
    "    \"Contingency.Cause\":\"The situations described in the arguments are causally influenced and the two are not in a conditional relation.\",\n",
    "    \"Contingency.Pragmatic cause\":\"Arg1 expresses a claim and Arg2 provides justification for this claim.\",\n",
    "    \"Contingency.Condition\": \"The situation in Arg2 is taken to be the condition and the situation described in Arg1 is taken to be the consequence.\",\n",
    "    \"Contingency.Pragmatic condition\": \"Used for instances of conditional constructions whose interpretation deviates from that of the semantics of “Condition”.\",\n",
    "    \"Comparison\": \"A discourse relation is established between Arg1 and Arg2 in order to highlight prominent differences between the two situations.\",\n",
    "    \"Comparison.Contrast\": \"Arg1 and Arg2 share a predicate or property and a difference is highlighted with respect to the values assigned to the shared property.\",\n",
    "    \"Comparison.Pragmatic contrast\": \"A contrast between one of the arguments and an inference that can be drawn from the other, in many cases at the speech act level: The contrast is not between the situations described in Arg1 and Arg2.\",\n",
    "    \"Comparison.Concession\": \"One argument denotes a fact that triggers a set of potential consequences, while the other denies one or more of them.\",\n",
    "    \"Comparison.Pragmatic concession\": \"One argument denotes a fact that triggers a set of potential consequences, while the other denies one or more of them. The denial is not at the level of the situations described in Arg1 and Arg2, but rather at the level of inferences that can be drawn from them.\",\n",
    "    \"Expansion\": \"Expanding the discourse and move its narrative or exposition forward.\",\n",
    "    \"Expansion.Conjunction\":\"The situation described in Arg2 provides additional, discourse new, information that is related to the situation described in Arg1, but is not related to Arg1 in any of the ways described for other types of “EXPANSION”.\",\n",
    "    \"Expansion.Instantiation\":\"Arg1 evokes a set and Arg2 describes it in further detail, It may be a set of events, a set of reasons, or a generic set of events, behaviors, attitudes, etc.\",\n",
    "    \"Expansion.Restatement\":\"The semantics of Arg2 restates the semantics of Arg1. It is inferred that the situations described in Arg1 and Arg2 hold true at the same time.\",\n",
    "    \"Expansion.Alternative\":\"Two arguments denote alternative situations.\",\n",
    "    \"Expansion.Exception\":\"Arg2 specifies an exception to the generalization specified by Arg1. In other words, Arg1 is false because Arg2 is true, but if Arg2 were false, Arg1 would be true.\",\n",
    "    \"Expansion.List\":\"Arguments are members of a list, defined in the prior discourse.“List”does not require the situations specified in Arg1 and Arg2 to be directly related.\"\n",
    "}\n",
    "def write_json(file, data):\n",
    "    import json\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Data saved to {file}\")\n",
    "\n",
    "def write_alpaca_format(df, file_path):\n",
    "    alpaca_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        prompt = prompt_tmeplate.format(\n",
    "            arg1=row['arg1'],\n",
    "            arg2=row['arg2'],\n",
    "        )\n",
    "        sense = row['conn1sense1']\n",
    "        sense_lst = sense.split('.')\n",
    "        if len(sense_lst) > 1:\n",
    "            sense = '.'.join(sense_lst[:2])\n",
    "        alpaca_data.append(\n",
    "            {\n",
    "                \"instruction\": prompt,\n",
    "                \"input\": \"\",\n",
    "                \"output\": sense2df[sense] + f' Relation: {sense}' if len(df) > 9999 else sense,\n",
    "            }\n",
    "        )\n",
    "    write_json(file_path, alpaca_data)\n",
    "\n",
    "write_alpaca_format(df.train_df, \"/data/whsun/idrr/data/arg2def/pdtb2/aplaca/train.json\")\n",
    "write_alpaca_format(df.test_df, \"/data/whsun/idrr/data/arg2def/pdtb2/aplaca/test.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
